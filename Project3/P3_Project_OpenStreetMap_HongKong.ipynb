{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import BeautifulSoup as soup\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Case Study\n",
    "\n",
    "## Map Area\n",
    "\n",
    "### Hong Kong, China\n",
    "\n",
    "http://www.openstreetmap.org/relation/913110#map=10/22.3526/114.1603\n",
    "\n",
    "Downloaded File: https://s3.amazonaws.com/metro-extracts.mapzen.com/hong-kong_china.osm.bz2 from the below site:\n",
    "https://mapzen.com/data/metro-extracts/\n",
    "\n",
    "I am living in Hong Kong and therefore I want to explore OSM data in my hometown and I am more familiar with the place that I can find error easier. I would like to contribute to improve the OSM data.\n",
    "\n",
    "reference: https://gist.github.com/carlward/54ec1c91b62a5f911c42#file-sample_project-md\n",
    "The reference project from Udacity have guided me through the processes, which is very useful for starting the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems Encountered in the Map\n",
    "\n",
    "### Dataset : Hong Kong\n",
    "\n",
    "* Chinese Character tag, however I decided not to deal with the unicode as it is impossible to clear the data completely, I will focus on the english tag instead. ( uncleaned)\n",
    "\n",
    "` '\\t\\t<tag k=\"name:zh\" v=\"\\xe9\\xa6\\x99\\xe6\\xb8\\xaf\\xe5\\x9c\\x8b\\xe9\\x9a\\x9b\\xe6\\xa9\\x9f\\xe5\\xa0\\xb4\"/>\\n' `\n",
    "\n",
    "* unknown abbreviation   (unclean, unknown features)\n",
    "\n",
    " 'LL': set(['LL'])\n",
    "\n",
    "* abbreviation ( cleaned) \n",
    "\n",
    "```\n",
    "\n",
    " u'Rd': set(['1 Stewart Rd',\n",
    "             '24 Tin Kwong Rd',\n",
    "```\n",
    "*  Included place not in Hong Kong. ( partly cleaned)(element.remove())\n",
    "\n",
    "```\n",
    "'Lu': set(['Hongbao Lu',\n",
    "            'Hua Fa Bei Lu',\n",
    "            'Shenyan Lu',\n",
    "            'XiHuan Lu',\n",
    "            桃园路 Taoyuan Lu,\n",
    "            沙河西路 ShaHe Xi Lu,\n",
    "            滨江东路 Binjiang Dong Lu]),\n",
    "```\n",
    "\n",
    "* irregular format of phone number (change to phone number start with +852 )\n",
    "\n",
    "```\n",
    "\n",
    "    ##set(['+852-2891-2231']) \n",
    "    ##set(['(852)26283888'])\n",
    "    ##set(['+85221470111'])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyPrettyPrinter(pprint.PrettyPrinter):\n",
    "    def format(self, object, context, maxlevels, level):\n",
    "        if isinstance(object, unicode):\n",
    "            return (object.encode('utf-8'), True, False)\n",
    "        return pprint.PrettyPrinter.format(self, object, context, maxlevels, level)\n",
    "##Set up a custom print function that can read chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<?xml version='1.0' encoding='UTF-8'?>\\n\"\n",
      "'<osm version=\"0.6\" generator=\"osmconvert 0.7T\" timestamp=\"2016-07-23T00:24:02Z\">\\n'\n",
      "'\\t<bounds minlat=\"21.591\" minlon=\"112.78\" maxlat=\"23.488\" maxlon=\"115.125\"/>\\n'\n",
      "'\\t<node id=\"274901\" lat=\"22.3460512\" lon=\"114.1811521\" version=\"7\" timestamp=\"2014-01-08T15:00:31Z\" changeset=\"19883770\" uid=\"1242214\" user=\"jc86035\"/>\\n'\n",
      "'\\t<node id=\"24323778\" lat=\"22.3176192\" lon=\"113.9359173\" version=\"10\" timestamp=\"2014-03-05T11:54:39Z\" changeset=\"20928659\" uid=\"1242214\" user=\"jc86035\"/>\\n'\n",
      "'\\t<node id=\"24323788\" lat=\"22.3127496\" lon=\"113.9377728\" version=\"6\" timestamp=\"2014-03-05T11:54:39Z\" changeset=\"20928659\" uid=\"1242214\" user=\"jc86035\"/>\\n'\n",
      "'\\t<node id=\"24323861\" lat=\"22.2947833\" lon=\"113.9376572\" version=\"8\" timestamp=\"2014-03-05T11:54:39Z\" changeset=\"20928659\" uid=\"1242214\" user=\"jc86035\"/>\\n'\n",
      "'\\t<node id=\"24323865\" lat=\"22.294111\" lon=\"113.9380887\" version=\"4\" timestamp=\"2014-03-05T11:54:39Z\" changeset=\"20928659\" uid=\"1242214\" user=\"jc86035\"/>\\n'\n",
      "'\\t<node id=\"24323870\" lat=\"22.2929343\" lon=\"113.9390522\" version=\"4\" timestamp=\"2014-03-05T11:54:39Z\" changeset=\"20928659\" uid=\"1242214\" user=\"jc86035\"/>\\n'\n",
      "'\\t<node id=\"24323874\" lat=\"22.2922756\" lon=\"113.9397474\" version=\"6\" timestamp=\"2014-03-05T11:54:39Z\" changeset=\"20928659\" uid=\"1242214\" user=\"jc86035\"/>\\n'\n"
     ]
    }
   ],
   "source": [
    "## get a sense of the data\n",
    "data='hong-kong_china.osm'\n",
    "with open(data,'r') as file:\n",
    "    x=0\n",
    "    for line in file:\n",
    "        if x<10:     \n",
    "                MyPrettyPrinter().pprint(line)\n",
    "                x=x+1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_tags(filename):\n",
    "    dict={}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "       if elem.tag not in dict.keys():\n",
    "           dict[elem.tag]=1\n",
    "       else:\n",
    "           dict[elem.tag]=dict[elem.tag]+1\n",
    "       elem.clear() ### The course just mention iterparse can save memory , not until running a 500Mb i find that we need to include this function.\n",
    "    return dict\n",
    "## to see what kind of tag exist\n",
    "tag_dict=count_tags(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node': 2574614, 'nd': 2984681, 'bounds': 1, 'member': 70957, 'tag': 868244, 'relation': 6304, 'way': 265372, 'osm': 1}\n"
     ]
    }
   ],
   "source": [
    "print tag_dict\n",
    "## These are the count of the tags\n",
    "## At this point, at least the tags look good that every tag type is expected.\n",
    "## I would like to take a look the tag inside the tag\"tag\". As many of the data are dominated by node without further tag\n",
    "## It is not easy to spot problem by print every line and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_k(filename):\n",
    "    dic={}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag in \"tag\":\n",
    "            if elem.attrib['k']:\n",
    "                if elem.attrib['k'] not in dic.keys():\n",
    "                    dic[elem.attrib['k']]=1\n",
    "                else:\n",
    "                    dic[elem.attrib['k']]=dic[elem.attrib['k']]+1\n",
    "    \n",
    "        elem.clear() ### The course just mention iterparse can save memory , not until running a 500Mb i find that we need to include this function.\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_list=count_k(data)\n",
    "## print tag_list       to see what kind of tag inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Function inherit from quiz\" Improving Street Name\" and I have modify it to fit the project.\n",
    "from collections import defaultdict\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd. \":\"Road\",\n",
    "            \"Ave\":\"Avenue\",\n",
    "            \"Rd.\":\"Road\",\n",
    "           \"Rd\":\"Road\",\n",
    "           'Ln':'Lane',\n",
    "           'Av':'Avenue',\n",
    "            }\n",
    "unwant={\"Lu\":\"Lu\"}\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def audit_phone_type(street_types, street_name):\n",
    "    m = phone_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "            \n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def is_phone_name(elem):\n",
    "    return (elem.attrib['k'] == \"phone\")\n",
    "\n",
    "phone_type_re = re.compile(r'\\(?\\d{852}\\)?|\\+852|\\+853|\\(?\\d{853}\\)?')\n",
    "\n",
    "def audit_street(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "        elem.clear()\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    phone_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_phone_name(tag):\n",
    "                    audit_phone_type(phone_types, tag.attrib['v'])\n",
    "                elif is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])     \n",
    "        elem.clear()\n",
    "    osm_file.close()\n",
    "    return street_types,phone_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    for map in mapping:\n",
    "        if map in name:\n",
    "           name = re.sub(r'\\b' + map + r'\\b\\.?', mapping[map], name)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st_types,phone_types = audit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##MyPrettyPrinter().pprint((dict(phone_types)))\n",
    "## Suppress printing for html output\n",
    "\n",
    "## To check any problematic building name\n",
    "##set(['+852-2891-2231'])\n",
    "##set(['(852)26283888'])\n",
    "##set(['+85221470111'])\n",
    "## these are sample of phone number, 852 is the phone region code for Hong Kong, if it is included in the data\n",
    "## I would like it start with\"+852\" . As the number of Hong Kong is 8 digit.\n",
    "\n",
    "\n",
    "\n",
    "## Phone is different to Street name, as they should be group by the first part of number (ie +852), instead of street name\n",
    "## that usually end with Street,Avenue,Road etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Some correction has been done for regular expression since the unicode of chinese does affect the original function,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "020-61972178\n",
      "+86 755 8958-3637\n",
      "+853 8791 9802\n",
      "+85235807319\n",
      "+853 2876 7668\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"sample.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        node_attribs = {}    \n",
    "        tags = []\n",
    "        \n",
    "        for item in NODE_FIELDS:\n",
    "            node_attribs[item] = element.attrib[item]\n",
    "\n",
    "        for tag in element.iter(\"tag\"):  \n",
    "            if is_street_name(tag):\n",
    "                if tag.attrib['v'] in unwant:\n",
    "                    element.remove(element)\n",
    "                    break\n",
    "                else:\n",
    "                    tag.attrib['v']=update_name(tag,mapping)\n",
    "            elif  is_phone_name(tag):\n",
    "                print tag.attrib['v']\n",
    "                if phone_type_re.search(tag.attrib['v']):\n",
    "                    tmp=re.split(r'\\(?\\d{852}\\)?|\\+852|\\+853|\\(?\\d{853}\\)?',tag.attrib['v'])\n",
    "                    tag.attrib['v']='+852'+' '+tmp[1]\n",
    "           \n",
    " \n",
    " \n",
    " \n",
    "            match_prob = PROBLEMCHARS.search(tag.attrib['k'])\n",
    "            if not match_prob:\n",
    "                node_tag_dict = {} \n",
    "                node_tag_dict['id'] = element.attrib['id'] \n",
    "                node_tag_dict['value'] = tag.attrib['v']  \n",
    "\n",
    "                m = LOWER_COLON.search(tag.attrib['k'])\n",
    "                if not m:\n",
    "                    node_tag_dict['type'] = 'regular'\n",
    "                    node_tag_dict['key'] = tag.attrib['k']\n",
    "                else:\n",
    "                    chars_before_colon = re.findall('^(.+?):', tag.attrib['k'])\n",
    "                    chars_after_colon = re.findall('^[a-z_]+:(.+)', tag.attrib['k'])\n",
    "\n",
    "                    node_tag_dict['type'] = chars_before_colon[0]\n",
    "                    node_tag_dict['key'] = chars_after_colon[0]\n",
    "                    \n",
    "            tags.append(node_tag_dict)\n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        way_attribs = {}\n",
    "        way_nodes = []\n",
    "        tags = []  \n",
    "    \n",
    "        for item in WAY_FIELDS:\n",
    "            way_attribs[item] = element.attrib[item]\n",
    "    \n",
    "        for tag in element.iter(\"tag\"):  \n",
    "            if is_street_name(tag):\n",
    "                if tag.attrib['v'] in unwant:\n",
    "                    element.remove(element)\n",
    "                    break ## since this element is not belong to Hong Kong, it should not be kept and directly jump out the loop.\n",
    "                else:\n",
    "                    tag.attrib['v']=update_name(tag,mapping)\n",
    "            elif is_phone_name(tag):\n",
    "                if phone_type_re.search(tag.attrib['v']):\n",
    "                    tmp=re.split(r'\\(?\\d{852}\\)?|\\+852|\\+853|\\(?\\d{853}\\)?',tag.attrib['v'])\n",
    "                    tag.attrib['v']='+852'+' '+tmp[1]\n",
    "           \n",
    " \n",
    "            match_prob = PROBLEMCHARS.search(tag.attrib['k'])\n",
    "            if not match_prob:\n",
    "                way_tag_dict = {}\n",
    "                way_tag_dict['id'] = element.attrib['id'] \n",
    "                way_tag_dict['value'] = tag.attrib['v']  \n",
    "\n",
    "                m = LOWER_COLON.search(tag.attrib['k'])\n",
    "                if not m:\n",
    "                    way_tag_dict['type'] = 'regular'\n",
    "                    way_tag_dict['key'] = tag.attrib['k']\n",
    "                else:\n",
    "                    chars_before_colon = re.findall('^(.+?):', tag.attrib['k'])\n",
    "                    chars_after_colon = re.findall('^[a-z_]+:(.+)', tag.attrib['k'])\n",
    "\n",
    "                    way_tag_dict['type'] = chars_before_colon[0]\n",
    "                    way_tag_dict['key'] = chars_after_colon[0]\n",
    "\n",
    "            tags.append(way_tag_dict)\n",
    "        count = 0\n",
    "        for tag in element.iter(\"nd\"):  \n",
    "            way_nd_dict = {} \n",
    "            \n",
    "            way_nd_dict['id'] = element.attrib['id'] \n",
    "            way_nd_dict['node_id'] = tag.attrib['ref'] \n",
    "            way_nd_dict['position'] = count  \n",
    "            count += 1\n",
    "            \n",
    "            way_nodes.append(way_nd_dict)\n",
    "    \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is False:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "\n",
    "# Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "# sample of the map when validating.\n",
    "process_map(OSM_PATH, validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## It seems work, no more error message.\n",
    " ##chars_after_colon = re.findall('^[a-z]+:(.+)', tag.attrib['k'])  has been modified to:\n",
    "    ##  chars_after_colon = re.findall('^[a-z_]+:(.+)', tag.attrib['k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL for this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-122-19b23891d7c6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-122-19b23891d7c6>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    1+!\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "##Connect to the database \n",
    "sqlite_file = 'my_db.db'    # name of the sqlite database file\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the table, specifying the column names and data types:\n",
    "cur.execute('''\n",
    "    CREATE TABLE nodes_tags(id INTEGER, key TEXT, value TEXT,type TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unicode_csv_reader(unicode_csv_data, dialect=csv.excel, **kwargs):\n",
    "    # csv.py doesn't do Unicode; encode temporarily as UTF-8:\n",
    "    csv_reader = csv.reader(utf_8_encoder(unicode_csv_data),\n",
    "                            dialect=dialect, **kwargs)\n",
    "    for row in csv_reader:\n",
    "        # decode UTF-8 back to Unicode, cell by cell:\n",
    "        yield [unicode(cell, 'utf-8') for cell in row]\n",
    "\n",
    "def utf_8_encoder(unicode_csv_data):\n",
    "    for line in unicode_csv_data:\n",
    "        yield line.encode('utf-8')\n",
    "\n",
    "def UnicodeDictReader(utf8_data, **kwargs):\n",
    "    csv_reader = csv.DictReader(utf8_data, **kwargs)\n",
    "    for row in csv_reader:\n",
    "        yield {key: unicode(value, 'utf-8') for key, value in row.iteritems()}\n",
    "##reference : https://discussions.udacity.com/t/sqlite-csv-import-problem/171479/2\n",
    "## utf_8_enconder to deal with the csv error\n",
    "##ProgrammingError: You must not use 8-bit bytestrings unless you use a text_factory that can interpret 8-bit bytestrings (like text_factory = str). It is highly recommended that you instead just switch your application to Unicode strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSVFILE = \"nodes_tags.csv\"\n",
    "\n",
    "with open (CSVFILE, 'rb') as csvfile:\n",
    "    # instead of using 'csv.DictReader' use `UnicodeDictReader`\n",
    "    csv_reader = UnicodeDictReader(csvfile)\n",
    "\n",
    "    to_db = [(i['id'], i['key'],i['value'], i['type']) for i in csv_reader]\n",
    "    # insert the formatted data\n",
    "cur.executemany(\"INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "cur.execute('''\n",
    "    CREATE TABLE nodes(id INTEGER PRIMARY KEY,                  \n",
    "lat FLOAT,\n",
    "lon FLOAT,\n",
    "user TEXT,\n",
    "uid  TEXT,\n",
    "version STRING,\n",
    "changeset INTEGER,\n",
    "timestamp STRING)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "CSVFILE = \"nodes.csv\"\n",
    "\n",
    "with open (CSVFILE, 'rb') as csvfile:\n",
    "    # instead of using 'csv.DictReader' use `UnicodeDictReader`\n",
    "    csv_reader = UnicodeDictReader(csvfile)\n",
    "\n",
    "    to_db = [(i['id'], i['lat'],i['lon'], i['user'],i['uid'],i['version'],i['changeset'],i['timestamp']) for i in csv_reader]\n",
    "    # insert the formatted data\n",
    "cur.executemany(\"INSERT INTO nodes(id, lat,lon,user,uid,version,changeset,timestamp) VALUES (?, ?, ?, ?, ?, ? , ? ,? );\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "cur.execute('''\n",
    "   CREATE TABLE ways(\n",
    "id INTEGER ,\n",
    "user TEXT,\n",
    "uid INTEGER,\n",
    "version TEXT,\n",
    "changeset INTEGER,\n",
    "timestamp TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "CSVFILE = \"ways.csv\"\n",
    "\n",
    "with open (CSVFILE, 'rb') as csvfile:\n",
    "    # instead of using 'csv.DictReader' use `UnicodeDictReader`\n",
    "    csv_reader = UnicodeDictReader(csvfile)\n",
    "\n",
    "    to_db = [(i['id'], i['user'],i['uid'], i['version'],i['changeset'],i['timestamp']) for i in csv_reader]\n",
    "    # insert the formatted data\n",
    "cur.executemany(\"INSERT INTO ways(id, user,uid,version,changeset,timestamp) VALUES (?, ?, ?, ? ,? ,?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "cur.execute('''\n",
    "   CREATE TABLE ways_nodes(\n",
    "id INTEGER ,\n",
    "node_id INTEGER,\n",
    "position INTEGER)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "CSVFILE = \"ways_nodes.csv\"\n",
    "\n",
    "with open (CSVFILE, 'rb') as csvfile:\n",
    "    # instead of using 'csv.DictReader' use `UnicodeDictReader`\n",
    "    csv_reader = UnicodeDictReader(csvfile)\n",
    "\n",
    "    to_db = [(i['id'], i['node_id'],i['position']) for i in csv_reader]\n",
    "    # insert the formatted data\n",
    "cur.executemany(\"INSERT INTO ways_nodes(id, node_id,position) VALUES (?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x12386b20>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('''DROP TABLE ways_nodes''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "cur.execute('''\n",
    "  CREATE TABLE ways_tags(\n",
    "id INTEGER ,\n",
    "key TEXT,\n",
    "value TEXT,\n",
    "type TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "CSVFILE = \"ways_tags.csv\"\n",
    "\n",
    "with open (CSVFILE, 'rb') as csvfile:\n",
    "    # instead of using 'csv.DictReader' use `UnicodeDictReader`\n",
    "    csv_reader = UnicodeDictReader(csvfile)\n",
    "\n",
    "    to_db = [(i['id'], i['key'], i['value'],i['type']) for i in csv_reader]\n",
    "    # insert the formatted data\n",
    "cur.executemany(\"INSERT INTO ways_tags(id, key,value,type) VALUES (?, ?, ? ,? );\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0xb55646c0>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "size of the file\n",
    "\n",
    "\n",
    "number of chosen type of nodes, like cafes, shops etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### size of the file\n",
    "```\n",
    " Date            Time       File size(MB)  File Name\n",
    "\n",
    "2016/07/29  下午 04:45       514      hong-kong_china.osm\n",
    "2016/07/31  下午 07:47       291      my_db.db\n",
    "2016/07/31  下午 10:18       2.03     nodes.csv\n",
    "2016/07/31  下午 10:18       67.3(KB) nodes_tags.csv\n",
    "2016/07/31  下午 10:18       155(KB)  ways.csv\n",
    "2016/07/31  下午 10:18       735(KB)  ways_nodes.csv\n",
    "2016/07/31  下午 10:18       240(KB)  ways_tags.csv\n",
    "  ```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of unique  user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of unique user:\n",
      "[(2824,)]\n"
     ]
    }
   ],
   "source": [
    "## count the total of distinct user ID\n",
    "\n",
    "cur.execute('''SELECT COUNT(DISTINCT(node_way.uid)) \n",
    "FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) node_way''')\n",
    "print\" Number of unique user:\" \n",
    "MyPrettyPrinter().pprint(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of nodes and ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of Nodes:\n",
      "[(2574614,)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('''SELECT COUNT(*) FROM nodes''')\n",
    "print\" Number of Nodes:\" \n",
    "MyPrettyPrinter().pprint(cur.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of Ways:\n",
      "[(265372,)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('''SELECT COUNT(*) FROM ways''')\n",
    "print\" Number of Ways:\"\n",
    "MyPrettyPrinter().pprint(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 city tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0xb55646c0>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(''' SELECT tags.value, COUNT(*) as count \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL \n",
    "      SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key LIKE '%city'\n",
    "GROUP BY tags.value\n",
    "ORDER BY count DESC\n",
    "LIMIT 10''') ## SHOW FIRST top 10 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten city tag :\n",
      "[(香港 Hong Kong, 363),\n",
      " (屯門 Tuen Mun, 128),\n",
      " (荃灣 Tsuen Wan, 78),\n",
      " (Hong Kong, 54),\n",
      " (紅磡 Hung Hom, 54),\n",
      " (Sai Kung, 53),\n",
      " (广东省深圳市, 50),\n",
      " (深圳, 42),\n",
      " (Ta Kwu Ling, 41),\n",
      " (Zhuhai, 39)]\n"
     ]
    }
   ],
   "source": [
    "print \"Top ten city tag :\"\n",
    "MyPrettyPrinter().pprint(cur.fetchall())\n",
    "\n",
    "## among the top 10 tags, 3 of them are not in Hong Kong, (广东省深圳市, 50), (Zhuhai, 39), (深圳, 42).\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 user:\n",
      "[(hlaw, 499316),\n",
      " (MarsmanRom, 245363),\n",
      " (Popolon, 160112),\n",
      " (sn0wblind, 120981),\n",
      " (katpatuka, 98086),\n",
      " (fsxy, 97258),\n",
      " (fdulezi, 84177),\n",
      " (KX675, 79145),\n",
      " (羊角忠实黑, 65681),\n",
      " (rainy3519446, 58215)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(''' SELECT e.user, COUNT(*) as num\n",
    "FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\n",
    "GROUP BY e.user\n",
    "ORDER BY num DESC\n",
    "LIMIT 10''')\n",
    "print \"Top 10 user:\"\n",
    "MyPrettyPrinter().pprint(cur.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Top 10 users contributions:\n",
      "[(1508334,)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('''SELECT sum(num)  \n",
    "FROM( SELECT  COUNT(*) as num\n",
    "FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\n",
    "GROUP BY e.user\n",
    "ORDER BY num DESC\n",
    "LIMIT 10)''')\n",
    "print \"Sum of Top 10 users contributions:\"\n",
    "top_10=cur.fetchall()\n",
    "MyPrettyPrinter().pprint(top_10)\n",
    "\n",
    "##Add SELECT sum(num), which the previous top 10 query become a subquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of  contributions:\n",
      "[(2839986,)]\n"
     ]
    }
   ],
   "source": [
    "## Total number of contributions\n",
    "cur.execute(''' SELECT count(*)\n",
    "FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\n",
    "\n",
    "''')\n",
    "print \"Total Number of  contributions:\"\n",
    "total=cur.fetchall()\n",
    "MyPrettyPrinter().pprint(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 users contributions to the total contribution: [ 53.11061393] %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print \"Top 10 users contributions to the total contribution:\" , np.array(top_10[0])*100.0/np.array(total[0]),\"%\"\n",
    "\n",
    "## As the return type is tuple, I need to transfrom it to np type before divide them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(yes, 2)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(''' SELECT value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "WHERE key='food'\n",
    "GROUP BY value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10'''\n",
    "            )\n",
    "MyPrettyPrinter().pprint(cur.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(name, 28365),\n",
      " (highway, 19008),\n",
      " (power, 15790),\n",
      " (en, 11218),\n",
      " (zh, 10801),\n",
      " (amenity, 8616),\n",
      " (place, 6030),\n",
      " (crossing, 4929),\n",
      " (hkbus, 3644),\n",
      " (railway, 3550),\n",
      " (barrier, 3276),\n",
      " (operator, 2777),\n",
      " (ref, 2758),\n",
      " (tourism, 2590),\n",
      " (type, 2471),\n",
      " (kmb, 2139),\n",
      " (natural, 1970),\n",
      " (shop, 1865),\n",
      " (shelter, 1622),\n",
      " (street, 1531)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(''' SELECT key, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "GROUP BY key\n",
    "ORDER BY num DESC\n",
    "LIMIT 20'''\n",
    "            )\n",
    "MyPrettyPrinter().pprint(cur.fetchall())\n",
    "### The higest 20 count keys\n",
    "### See what kind of key has the most tags, I see tag\"tourism\"  and \"shop\" which interest me, I will explore this 3 keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(information, 1301),\n",
      " (attraction, 433),\n",
      " (hotel, 259),\n",
      " (viewpoint, 162),\n",
      " (guest_house, 141),\n",
      " (picnic_site, 96),\n",
      " (artwork, 71),\n",
      " (hostel, 44),\n",
      " (camp_site, 43),\n",
      " (museum, 31),\n",
      " (motel, 3),\n",
      " (zoo, 3),\n",
      " (chalet, 1),\n",
      " (gallery, 1),\n",
      " (theme_park, 1)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(''' SELECT value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "WHERE key='tourism'\n",
    "GROUP BY value\n",
    "ORDER BY num DESC\n",
    "LIMIT 20'''\n",
    ")\n",
    "MyPrettyPrinter().pprint(cur.fetchall())\n",
    "### The 10 higest value with key=tourism\n",
    "###  I can see Hong Kong have relatively few hostel compare to hotel in this query. There are 259 hotel and 44 hostel in the \n",
    "### database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(toilets, 1115),\n",
      " (restaurant, 973),\n",
      " (shelter, 671),\n",
      " (bank, 598),\n",
      " (parking, 574),\n",
      " (bus_station, 573),\n",
      " (post_box, 572),\n",
      " (fast_food, 353),\n",
      " (cafe, 239),\n",
      " (fuel, 239)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(''' SELECT value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "WHERE key='amenity'\n",
    "GROUP BY value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10'''\n",
    ")\n",
    "MyPrettyPrinter().pprint(cur.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Again, it is funny that toliets have so many tags. \n",
    "## There are also so many banks tag that even more than bus station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(261189, 317),\n",
      " (169827, 257),\n",
      " (2238851, 80),\n",
      " (230481, 36),\n",
      " (460264, 31),\n",
      " (31685, 26),\n",
      " (1665943, 22),\n",
      " (204590, 16),\n",
      " (1879520, 15),\n",
      " (2385962, 15)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(''' SELECT nodes.uid, COUNT(*) as num\n",
    "FROM nodes\n",
    "INNER JOIN nodes_tags\n",
    "ON nodes.id=nodes_tags.id\n",
    "WHERE value='toilets'\n",
    "GROUP BY nodes.uid\n",
    "ORDER BY num DESC\n",
    "LIMIT 10'''\n",
    ")\n",
    "MyPrettyPrinter().pprint(cur.fetchall())\n",
    "### Select user id and count who make the tag \"toilets\" by joining nodes_tags with nodes which share the same id\n",
    "### It appear that 2 users \"261189\" and \"169827\" work quite hard to tag a lot of toilets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(convenience, 491),\n",
      " (supermarket, 377),\n",
      " (clothes, 140),\n",
      " (mall, 87),\n",
      " (bakery, 68),\n",
      " (books, 41),\n",
      " (chemist, 41),\n",
      " (jewelry, 40),\n",
      " (bicycle, 33),\n",
      " (kiosk, 33),\n",
      " (department_store, 29),\n",
      " (yes, 29),\n",
      " (electronics, 28),\n",
      " (car, 27),\n",
      " (hairdresser, 26),\n",
      " (gift, 22),\n",
      " (butcher, 20),\n",
      " (greengrocer, 20),\n",
      " (confectionery, 19),\n",
      " (mobile_phone, 19)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(''' SELECT value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "WHERE key='shop'\n",
    "GROUP BY value\n",
    "ORDER BY num DESC\n",
    "LIMIT 20'''\n",
    ")\n",
    "MyPrettyPrinter().pprint(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## There are many convenience tags, indeed Hong Kong have many convenience shops like 7-11, I believe the number is larger than \n",
    "## that. It surprise me supermarket have the second largest number of tag, I doubt it is the second common shops in Hong Kong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "I think the Hong Kong dataset is incomplete. There are also some data belong to ShenZhen which is not inside Hong Kong. There are some users which have a great contribution to the dataset as I have seen. I am surprised that toliets have the most tags for amenity, I believe there are more restaurant than toilets in Hong Kong, there are still lots of thing to add into the dataset!\n",
    "\n",
    "I have not tried to clean the chinese character for this project as I have spent some many time to input the data, it is painful that I can not output the correct data and put it into database. I believe the process should be similar but there are more work spend on dealing with unicode instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the toilets tag surpurise me, I also think it may be a potential problem that certain users dominate the dataset, it will be better if more people participate and validate the data. It will be misleading if I say toilet is the most common amenity in Hong Kong and it certainly bring some problem to analysis. There are also many node with no tags while some of them have many tag. While it is a open dataset, is that possible to make a competition on some web like Kaggle to motivate more people participate in the dataset to help clean it?\n",
    "I did not use OSM before, I am thinking that sometime there are many repeats tag for the same node or way, is that possible people can remove the tag freely? If not, is that possible people can give \"likes\" for a tag that the site will rank the top \"likes\" tag that people can figure out which tag is more reliable when there is more than one tag? It could be a problem if people can freely remove others tags, I think an option to \"like\" a tag and rank it may be the best practical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
