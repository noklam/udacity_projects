{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pca__n_components': 4, 'selection__k': 8, 'clf__learning_rate': 0.001, 'clf__n_estimators': 100}\n",
      "make_scorer(custom_scorer)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.93      0.95        40\n",
      "        1.0       0.50      0.75      0.60         4\n",
      "\n",
      "avg / total       0.93      0.91      0.92        44\n",
      "\n",
      "\n",
      "\n",
      "And these are the results going through the test classifier:\n",
      "\n",
      "Pipeline(steps=[('std', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=8, score_func=<function f_classif at 0x0000000009F909E8>)), ('pca', PCA(copy=True, n_components=4, whiten=False)), ('clf', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifie...dom_state=None, splitter='best'),\n",
      "          learning_rate=0.001, n_estimators=100, random_state=0))])\n",
      "\tAccuracy: 0.84527\tPrecision: 0.40243\tRecall: 0.33100\tF1: 0.36324\tF2: 0.34318\n",
      "\tTotal predictions: 15000\tTrue positives:  662\tFalse positives:  983\tFalse negatives: 1338\tTrue negatives: 12017\n",
      "\n",
      "186.04700017\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jul 20 20:25:30 2016\n",
    "\n",
    "@author: Noklam\n",
    "\"\"\"\n",
    "\n",
    "## This Project is greatly inspired by the discussion, which help me a lot to \n",
    "## debug and I have give up my original code and follow their structure and \n",
    "## made my own classifier \n",
    "## Especially helpful discussion from vivek_29420285151271 and michael_807478\n",
    "## As I am not a pro-programmer, doesnt develop habit of writing nice function\n",
    "## Their well-written function have helped me a lot to simplify the final \n",
    "## version\n",
    "######Steps:\n",
    "# 1. Remove outlier \"TOTAL\"\n",
    "# 2. Remove unwanted features\n",
    "# 3. Log transformation\n",
    "# 4. Pipeline\n",
    "# 5. Imputer 0 for NaN, MinMaxScaler\n",
    "# 6. Select K-best , K=8\n",
    "# 7. PCA ,choose first 4 \n",
    "# 8. Run Classifier AdaBoostClassifier,\n",
    "# 9. Precision =0.4 ,Recall =0.33\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jul 20 20:25:30 2016\n",
    "\n",
    "@author: Noklam\n",
    "\"\"\"\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "import matplotlib.pyplot as plt\n",
    "## allow graph to be displayed\n",
    "import pylab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.preprocessing import Imputer                                            \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn import cross_validation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from tester import test_classifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "def data_preprocessing(data):\n",
    "    \n",
    "   \n",
    "    with open(data, \"r\") as data_file:\n",
    "        data_dict = pickle.load(data_file)\n",
    "    df = pd.DataFrame.from_dict(data_dict, orient='index', dtype=np.float)\n",
    "    df = df.drop('TOTAL') ## remove \n",
    "\n",
    "    ### Drop all the variable that I have done before, this function is aim to repeat the above process in a nice function.\n",
    "    df = df.drop('email_address', axis=1)\n",
    "    df = df.astype(float)\n",
    "    drop_list=['deferral_payments', 'deferred_income', 'director_fees', 'loan_advances', 'restricted_stock_deferred']\n",
    "    df=df.drop(drop_list, axis=1)\n",
    "    ###new variable added\n",
    "    df['to_ratio'] = df['from_poi_to_this_person']/(df['from_messages'] )\n",
    "    df['from_ratio'] = df['from_this_person_to_poi']/(df['from_messages'])\n",
    "    df=df.drop('from_this_person_to_poi',axis=1)\n",
    "    df=df.drop('from_poi_to_this_person',axis=1)\n",
    "    df=df.drop('from_messages',axis=1)\n",
    "    df=df.drop('to_messages',axis=1)\n",
    "    \n",
    "    labels=df['poi']\n",
    "    df=df.drop('poi',axis=1)\n",
    "    df.replace(to_replace='NaN', value=np.nan, inplace=True)\n",
    "    df=np.log(df+1)\n",
    "    features_list=list(df.columns.values)\n",
    "    \n",
    "    df.insert(0,'poi',labels)\n",
    "    df=df.fillna(0)\n",
    "    features=df[features_list]\n",
    "    features_list.insert(0,'poi')\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    return data_dict,df, features, labels, features_list\n",
    "def get_train_test_split(features, labels):\n",
    "    '''This gets the train test split for the sklearn runs of the model'''\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    return features_train, features_test, labels_train, labels_test\n",
    "\n",
    "def custom_scorer(labels, predictions):\n",
    "    precision = precision_score(labels,predictions)\n",
    "    recall = recall_score(labels,predictions )\n",
    "    min_score = min(precision,recall)\n",
    "    return min_score\n",
    "    \n",
    "scorer  = make_scorer(custom_scorer, greater_is_better=True)\n",
    "\n",
    "data=\"final_project_dataset.pkl\"\n",
    "data_dict,df, features, labels, features_list = data_preprocessing(data)\n",
    "\n",
    "features_train, features_test, labels_train, labels_test=get_train_test_split( features,labels)\n",
    "\n",
    "## I choose AdaboostClassifier to be my final choice as it seems to provide the highest performance\n",
    "## and I have tune the parameter which perform best in the training sample\n",
    "Pipeline_final = Pipeline([\n",
    "        ('std', MinMaxScaler()),\n",
    "        ('selection', SelectKBest()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),random_state=0))\n",
    "    ])\n",
    "# estimator parameters\n",
    "    #'\n",
    "k = [8]\n",
    "c = [4]\n",
    "e = [100]\n",
    "r = [0.001]\n",
    "\n",
    "\n",
    "param_grid={'selection__k': k,\n",
    "            'pca__n_components': c,\n",
    "            'clf__n_estimators': e,\n",
    "            'clf__learning_rate': r,\n",
    "             }\n",
    "\n",
    "# The imputer is useless as I end up fill all NaN with 0 which give me higher accuracy          \n",
    "# set model parameters to grid search object\n",
    "gridCV_object_final = GridSearchCV(estimator = Pipeline_final, \n",
    "                             param_grid = param_grid,\n",
    "                             scoring = scorer,\n",
    "                             cv = StratifiedShuffleSplit(labels_train,200,train_size=0.1,random_state=12))\n",
    "\n",
    "# train the model\n",
    "gridCV_object_final.fit(features_train, labels_train)\n",
    "\n",
    "print gridCV_object_final.best_params_\n",
    "print gridCV_object_final.scorer_\n",
    "pred_CV = gridCV_object_final.predict(features_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(labels_test, pred_CV)\n",
    "            \n",
    "from  time import time\n",
    "\n",
    "df1 = df.transpose()\n",
    "df1 = df1.to_dict()\n",
    "print \"\\n\\nAnd these are the results going through the test classifier:\\n\"\n",
    "## send the best parameter to clf\n",
    "t0=time()\n",
    "clf=gridCV_object_final.best_estimator_\n",
    "test_classifier(clf, df1, features_list, folds = 1000)\n",
    "t1=time()-t0\n",
    "print t1\n",
    "\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "dump_classifier_and_data(clf,df1,features_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_ratio</th>\n",
       "      <th>from_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALLEN PHILLIP K</th>\n",
       "      <td>15.244625</td>\n",
       "      <td>14.363367</td>\n",
       "      <td>9.537411</td>\n",
       "      <td>12.627431</td>\n",
       "      <td>5.030438</td>\n",
       "      <td>11.744259</td>\n",
       "      <td>12.215805</td>\n",
       "      <td>7.249926</td>\n",
       "      <td>15.316125</td>\n",
       "      <td>14.363367</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.029183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.460009</td>\n",
       "      <td>8.156797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.114325</td>\n",
       "      <td>12.460009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BANNANTINE JAMES M</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.213278</td>\n",
       "      <td>10.938485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.669934</td>\n",
       "      <td>14.379433</td>\n",
       "      <td>6.169611</td>\n",
       "      <td>6.144186</td>\n",
       "      <td>13.727988</td>\n",
       "      <td>15.472497</td>\n",
       "      <td>0.852212</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>13.997833</td>\n",
       "      <td>15.714710</td>\n",
       "      <td>9.323758</td>\n",
       "      <td>14.276761</td>\n",
       "      <td>14.793951</td>\n",
       "      <td>15.187380</td>\n",
       "      <td>12.495390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.544391</td>\n",
       "      <td>16.178556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAY FRANKLIN R</th>\n",
       "      <td>12.899222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.768676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.248495</td>\n",
       "      <td>11.889971</td>\n",
       "      <td>12.387027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.626402</td>\n",
       "      <td>11.051128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        bonus  exercised_stock_options   expenses  \\\n",
       "ALLEN PHILLIP K     15.244625                14.363367   9.537411   \n",
       "BADUM JAMES P        0.000000                12.460009   8.156797   \n",
       "BANNANTINE JAMES M   0.000000                15.213278  10.938485   \n",
       "BAXTER JOHN C       13.997833                15.714710   9.323758   \n",
       "BAY FRANKLIN R      12.899222                 0.000000  11.768676   \n",
       "\n",
       "                    long_term_incentive      other  restricted_stock  \\\n",
       "ALLEN PHILLIP K               12.627431   5.030438         11.744259   \n",
       "BADUM JAMES P                  0.000000   0.000000          0.000000   \n",
       "BANNANTINE JAMES M             0.000000  13.669934         14.379433   \n",
       "BAXTER JOHN C                 14.276761  14.793951         15.187380   \n",
       "BAY FRANKLIN R                 0.000000   4.248495         11.889971   \n",
       "\n",
       "                       salary  shared_receipt_with_poi  total_payments  \\\n",
       "ALLEN PHILLIP K     12.215805                 7.249926       15.316125   \n",
       "BADUM JAMES P        0.000000                 0.000000       12.114325   \n",
       "BANNANTINE JAMES M   6.169611                 6.144186       13.727988   \n",
       "BAXTER JOHN C       12.495390                 0.000000       15.544391   \n",
       "BAY FRANKLIN R      12.387027                 0.000000       13.626402   \n",
       "\n",
       "                    total_stock_value  to_ratio  from_ratio  \n",
       "ALLEN PHILLIP K             14.363367  0.021186    0.029183  \n",
       "BADUM JAMES P               12.460009  0.000000    0.000000  \n",
       "BANNANTINE JAMES M          15.472497  0.852212    0.000000  \n",
       "BAXTER JOHN C               16.178556  0.000000    0.000000  \n",
       "BAY FRANKLIN R              11.051128  0.000000    0.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'bonus',\n",
       " 'exercised_stock_options',\n",
       " 'expenses',\n",
       " 'long_term_incentive',\n",
       " 'other',\n",
       " 'restricted_stock',\n",
       " 'salary',\n",
       " 'shared_receipt_with_poi',\n",
       " 'total_payments',\n",
       " 'total_stock_value',\n",
       " 'to_ratio',\n",
       " 'from_ratio']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>bonus</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_ratio</th>\n",
       "      <th>from_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.124138</td>\n",
       "      <td>7.574308</td>\n",
       "      <td>9.661799</td>\n",
       "      <td>6.729323</td>\n",
       "      <td>5.852428</td>\n",
       "      <td>6.318502</td>\n",
       "      <td>9.831722</td>\n",
       "      <td>8.007985</td>\n",
       "      <td>3.728530</td>\n",
       "      <td>11.544968</td>\n",
       "      <td>12.004710</td>\n",
       "      <td>0.306178</td>\n",
       "      <td>0.091930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.330882</td>\n",
       "      <td>6.790419</td>\n",
       "      <td>6.536471</td>\n",
       "      <td>5.088038</td>\n",
       "      <td>6.546970</td>\n",
       "      <td>5.458204</td>\n",
       "      <td>5.861261</td>\n",
       "      <td>5.961404</td>\n",
       "      <td>3.344199</td>\n",
       "      <td>5.101617</td>\n",
       "      <td>5.128449</td>\n",
       "      <td>0.519126</td>\n",
       "      <td>0.147079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.419647</td>\n",
       "      <td>12.306560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.611541</td>\n",
       "      <td>13.317664</td>\n",
       "      <td>9.843472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.854355</td>\n",
       "      <td>12.795328</td>\n",
       "      <td>12.257246</td>\n",
       "      <td>4.744932</td>\n",
       "      <td>13.727988</td>\n",
       "      <td>13.770381</td>\n",
       "      <td>0.020502</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.592368</td>\n",
       "      <td>14.327292</td>\n",
       "      <td>10.880365</td>\n",
       "      <td>12.832941</td>\n",
       "      <td>11.921446</td>\n",
       "      <td>13.457293</td>\n",
       "      <td>12.502753</td>\n",
       "      <td>6.803505</td>\n",
       "      <td>14.475287</td>\n",
       "      <td>14.640900</td>\n",
       "      <td>0.454255</td>\n",
       "      <td>0.181017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.894952</td>\n",
       "      <td>17.352066</td>\n",
       "      <td>12.340446</td>\n",
       "      <td>15.453620</td>\n",
       "      <td>16.153437</td>\n",
       "      <td>16.507546</td>\n",
       "      <td>13.921004</td>\n",
       "      <td>8.616495</td>\n",
       "      <td>18.455660</td>\n",
       "      <td>17.709575</td>\n",
       "      <td>2.527632</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              poi       bonus  exercised_stock_options    expenses  \\\n",
       "count  145.000000  145.000000               145.000000  145.000000   \n",
       "mean     0.124138    7.574308                 9.661799    6.729323   \n",
       "std      0.330882    6.790419                 6.536471    5.088038   \n",
       "min      0.000000    0.000000                 0.000000    0.000000   \n",
       "25%      0.000000    0.000000                 0.000000    0.000000   \n",
       "50%      0.000000   12.611541                13.317664    9.843472   \n",
       "75%      0.000000   13.592368                14.327292   10.880365   \n",
       "max      1.000000   15.894952                17.352066   12.340446   \n",
       "\n",
       "       long_term_incentive       other  restricted_stock      salary  \\\n",
       "count           145.000000  145.000000        145.000000  145.000000   \n",
       "mean              5.852428    6.318502          9.831722    8.007985   \n",
       "std               6.546970    5.458204          5.861261    5.961404   \n",
       "min               0.000000    0.000000          0.000000    0.000000   \n",
       "25%               0.000000    0.000000          0.000000    0.000000   \n",
       "50%               0.000000    6.854355         12.795328   12.257246   \n",
       "75%              12.832941   11.921446         13.457293   12.502753   \n",
       "max              15.453620   16.153437         16.507546   13.921004   \n",
       "\n",
       "       shared_receipt_with_poi  total_payments  total_stock_value    to_ratio  \\\n",
       "count               145.000000      145.000000         145.000000  145.000000   \n",
       "mean                  3.728530       11.544968          12.004710    0.306178   \n",
       "std                   3.344199        5.101617           5.128449    0.519126   \n",
       "min                   0.000000        0.000000           0.000000    0.000000   \n",
       "25%                   0.000000       11.419647          12.306560    0.000000   \n",
       "50%                   4.744932       13.727988          13.770381    0.020502   \n",
       "75%                   6.803505       14.475287          14.640900    0.454255   \n",
       "max                   8.616495       18.455660          17.709575    2.527632   \n",
       "\n",
       "       from_ratio  \n",
       "count  145.000000  \n",
       "mean     0.091930  \n",
       "std      0.147079  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      0.181017  \n",
       "max      0.693147  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "The goal of this project is to identify the POI from the given data, machine learning is able to dig into the data and extract some useful information. The dataset mainly contain two part of data, financial features: salary, bonus , etc and email features: email_address, from_message, to message etc.\n",
    "\n",
    "There is a data called\"TOTAL\" which is an outlier that sum up all other data, I have removed it since the beginning of the analysis. There are also quite a lot missing data in the dataset. I have remove the features which have NaN larger than a threshold, I have try to remove more features but turn out I keep my first try as it give the greatest performence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data point : 146\n",
      "Number of POI :  18\n",
      "Number of non_POI :  128\n",
      "This number of POI and non-POI is before removing outlier TOTAL\n",
      "Number of features used : 12\n",
      "================================================================\n",
      "Number of Missing data for each features:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "salary                        51\n",
       "to_messages                   59\n",
       "deferral_payments            107\n",
       "total_payments                21\n",
       "exercised_stock_options       44\n",
       "bonus                         64\n",
       "restricted_stock              36\n",
       "shared_receipt_with_poi       59\n",
       "restricted_stock_deferred    128\n",
       "total_stock_value             20\n",
       "expenses                      51\n",
       "loan_advances                142\n",
       "from_messages                 59\n",
       "other                         53\n",
       "from_this_person_to_poi       59\n",
       "poi                            0\n",
       "director_fees                129\n",
       "deferred_income               97\n",
       "long_term_incentive           80\n",
       "email_address                  0\n",
       "from_poi_to_this_person       59\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##total number of data points\n",
    "print \"Number of data point :\" ,len(data_dict)\n",
    "##allocation across classes (POI/non-POI)\n",
    "\n",
    "count_poi=0\n",
    "for name in data_dict:\n",
    "    count_poi=count_poi+data_dict[name]['poi']  ## since poi is either 1 or 0, sum of it is the total numnber of poi\n",
    "count_non_poi=len(data_dict)-count_poi\n",
    "print \"Number of POI : \", count_poi\n",
    "print \"Number of non_POI : \", count_non_poi\n",
    "print \"This number of POI and non-POI is before removing outlier TOTAL\"\n",
    "\n",
    "##number of features used\n",
    "print\"Number of features used :\", len(features_list)-1 ### poi is not included for features used.\n",
    "##are there features with many missing values? etc.\n",
    "print\"========\"*8\n",
    "print\"Number of Missing data for each features:\"\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deferral_payments' 'restricted_stock_deferred' 'loan_advances'\n",
      " 'director_fees' 'deferred_income']\n"
     ]
    }
   ],
   "source": [
    "drop_list=df.isnull().sum().index.values[ np.array(df.isnull().sum() > 80) ]\n",
    "print drop_list\n",
    "### Please note that the features which have more than 80 missing data are removed and the dropped featured are listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]\n",
    "\n",
    "The features I kept is shown below, I have do MinMaxScaler scaling as I want to perform PCA and the unit for different features is different, it will be dominant by some of the features if I don't scale it first.\n",
    "I have create two features, to_ratio and from_ratio , which is correspond to from_poi_to_this_person/from_messages and from_this_person_to_poi/to_messages. The reason behind is that I think for the potential POI, they will have more communication within their community than other normal people. A single measurement from POI or to POI may have bias as certain people may really need to communicate to them more frequently. Thus, I scale it to the from_messages and to_messages which imples not the absolute amount of messages, but the portion of their message to/from poi.\n",
    "I have use pipeline and gridsearchCV to tune the parameter, all features excepted the dropped features is pass to the pipepline.\n",
    "There are 4 steps in the pipeline:\n",
    "1. MinMaxScaler\n",
    "2. SelectKBest\n",
    "3. PCA\n",
    "4. Classifier\n",
    "The optimized parameter is selected, which the 8 best features are selected by the SelectKBest and the first 4 PCs is choosed by PCA. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['bonus', 'expenses', 'other', 'salary', 'shared_receipt_with_poi',\n",
       "       'total_payments', 'to_ratio', 'from_ratio'], \n",
       "      dtype='|S23')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print\"Selected Features:\",\"===\"*20\n",
    "selected=clf.named_steps['selection'].get_support()\n",
    "features_list.remove('poi')\n",
    "np.array(features_list)[selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.31315372,   0.02858096,  12.93526457,   3.29949313,\n",
       "        16.87507603,   5.39384063,   8.84385607,   5.13845767,\n",
       "         6.06693833,   5.44001362,  12.04371759,  16.0935593 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.named_steps['selection'].scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summary of the performence on the algorithm with \"to_ratio\" and \"from_ratio\" added.\n",
    "Accuracy: 0.84580\tPrecision: 0.40440\tRecall: 0.33100\tF1: 0.36404\tF2: 0.34347\n",
    "\tTotal predictions: 15000\tTrue positives:  662\tFalse positives:  975\tFalse negatives: 1338\tTrue negatives: 12025\n",
    "#### without the new features added\n",
    "Accuracy: 0.82340\tPrecision: 0.30878\tRecall: 0.26200\tF1: 0.28347\tF2: 0.27019\n",
    "\tTotal predictions: 15000\tTrue positives:  524\tFalse positives: 1173\tFalse negatives: 1476\tTrue negatives: 11827"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "I use SelectKBest then PCA and AdaBoostClassifier. I have also try GradientBoosting GaussianNB, SVC. I find Adaboost give me the highest performance then I keep it as my choice. The performence is shown below.\n",
    "\n",
    "Supplimentary answer: I should explain this part more, for the highest performance I mentioned, I mean the training classifier performence not the test classifier. The procedures I take is 1. run different algorithm with different parameter using Pipeline and GridsearchCV . 2. pick the classifier which give me the highest performance in the training set. 3. Add some more parameter to test the classifier I choosed( As if i try too many parameter for each classifier it takes me too much time, so I just test with less parameter choice at first) 4. Build the final classifier based on the chose classifier.\n",
    "\n",
    "Please correct me if I am wrong, I just want to make sure my concept is right.\n",
    "When we are building a classifier, we SHOULD train the classifier with train/test split. A furhter train/cross-validation split in the training set is done and we should choose the classifier works best on which have the best performence in the cross-validation test. The classifier then pass to the testing set to get the final performance.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###These are all algorithms' performence on training set: \n",
    "### GradientBoostingClassifier\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "      False       0.93      0.93      0.93        40\n",
    "       True       0.25      0.25      0.25         4\n",
    "\n",
    "avg / total       0.86      0.86      0.86        44\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### AdaBoostClassifier\n",
    "    precision    recall  f1-score   support\n",
    "\n",
    "      False       0.95      0.93      0.94        40\n",
    "       True       0.40      0.50      0.44         4\n",
    "\n",
    "avg / total       0.90      0.89      0.89        44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  ####SVC\n",
    "    precision    recall  f1-score   support\n",
    "\n",
    "      False       0.92      0.88      0.90        40\n",
    "       True       0.17      0.25      0.20         4\n",
    "\n",
    "avg / total       0.85      0.82      0.83        44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###GaussianNB\n",
    "       precision    recall  f1-score   support\n",
    "\n",
    "      False       0.91      0.97      0.94        40\n",
    "       True       0.00      0.00      0.00         4\n",
    "\n",
    "avg / total       0.82      0.89      0.85        44\n",
    "\n",
    "### I am very confused on why it can not predict a single true POI, as I have seen people using this algorithm on the discussion\n",
    "### forum, I did some work to remove some features and try this algorithm seperately but still very poor performence, the \n",
    "### process is not recorded properly as I have just did it in the console and lost the record after. Any insight why this \n",
    "### classifier work so bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### AdaBoostClassifier\n",
    "  precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.97      0.93      0.95        40\n",
    "        1.0       0.50      0.75      0.60         4\n",
    "\n",
    "avg / total       0.93      0.91      0.92        44\n",
    "## As the performence is the best among the classifiers, therefore I choose it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]\n",
    "Tuning the parameters is analogous to tune a TV cable, you may fail to watch the tv channel if you do not do so, or get bad quality. It is important to fine tune the classifier to maximize the performence. I use GridSearchCV did a lot of testing on max depth, imputer, learning rate, pca  and selectedKbest features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]\n",
    "Classic mistake is overfitting the model, if a cross-validation test is not used, you can always get a classifier that have 100% accuracy, as it is basically doing an exam with answer provided. \n",
    "\n",
    "First,70% data is split as training data and 30% data is testing data. Then a Stratifiedshufflesplit it used to split the training set to training and testing set(cross-validation set), the testing set is the cross validation set that the gridsearchCV selected the best parameter base on data train and predict in the validation set.\n",
    "The best estimator is then used to submit to the grader for final testing.\n",
    "\n",
    "StratifiedShuffleSplit is used because the dataset is relatively small and POI is only accounted for a small number. A StratifiedShuffleSplit is to make sure the training set have POI in it by keeping the percentage of class as otherwise it can rate bad classifier which always classify Non-POI as a good classifier.\n",
    "\n",
    "If no cross-validation is done, you can always overfitting by trial and error and you eventually get a classifier that can predict accurately on the particular testing set but which is very likely not working to future data.\n",
    "\n",
    "For recall and precision, I take the performence of my selected classifier as an example.\n",
    "Precision and recall both take the range [0 1].\n",
    "Precision = True positive/ ( True Positive + False Positive)\n",
    "Recall = True positive/ ( True Positive + False Negative)\n",
    "\n",
    "For precision, it is a summary on the classifier where the proportion of True classification is right. In the example, precision =0.402 means  when 100 True classifications is made, 40.2 is positive true classification.\n",
    "\n",
    "For recall, it is a summary on the classifier where the proportion of True classification is predicted by the classifier. in th eexample, recall = 0.331. It mean when there are 100 True labels, 33.1 of them will be picked up by the classifier.\n",
    "\n",
    "A high precision indicate that whenever the classifier make a True classification, it is very likely to be correct.\n",
    "A high recall indicate that the classifier is able to classify most of the positive True labels from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### AdaBoostClassifier\n",
    "Accuracy: 0.84527\tPrecision: 0.40243\tRecall: 0.33100\tF1: 0.36324\tF2: 0.34318\n",
    "\tTotal predictions: 15000\tTrue positives:  662\tFalse positives:  983\tFalse negatives: 1338\tTrue negatives: 12017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
